{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <br>\n",
    "\n",
    "The very young AgriTech start-up, named “**Fruits**!”,\n",
    "seeks to offer innovative solutions for fruit harvesting.\n",
    "\n",
    "The company's desire is to preserve the biodiversity of fruits\n",
    "by allowing specific treatments for each species of fruit\n",
    "by developing intelligent picking robots.\n",
    "\n",
    "The start-up initially wishes to make itself known by putting\n",
    "available to the general public a mobile application which would allow\n",
    "users to take a photo of a fruit and get information about that fruit.\n",
    "\n",
    "For the start-up, this application would raise awareness among the general public\n",
    "to fruit biodiversity and to set up a first version of the engine\n",
    "classification of fruit images.\n",
    "\n",
    "In addition, the development of the mobile application will make it possible to build\n",
    "a first version of the **Big Data** architecture required.\n",
    "\n",
    "## Objectives in this project\n",
    "\n",
    "1. Develop a first data processing chain that <br />\n",
    "   will include **preprocessing** and a **dimension reduction** step.\n",
    "2. Take into account that <u>the volume of data will increase <br />\n",
    "   very quickly</u>after delivery of this project, which involves:\n",
    " - Deploy data processing in a **Big Data** environment\n",
    " - Develop scripts in **pyspark** to perform **distributed computing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical choices <br>\n",
    "We must take into account the very rapid increase in the volume of data after delivery of the project. This is why it is important to introduce a distributed calculation methodology upstream. To do this we will create **pyspark** scripts. <br>\n",
    "**pySpark** is a way to communicate\n",
    "with **Spark** via the **Python** language.<br />\n",
    "**Spark**, for its part, is a tool that allows you to manage and coordinate\n",
    "performing tasks on data across a group of computers. <br />\n",
    "<u>Spark (or Apache Spark) is an open source distributed computing framework <br />\n",
    "in-memory for processing and analyzing massive data</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Schéma de Spark](img/spark-schema.png)\n",
    "\n",
    "*The driver (sometimes called “Spark Session”) distributes and schedules\n",
    "the tasks between the different executors which execute them and allow\n",
    "distributed processing. He is responsible for executing the code\n",
    "on the different machines.\n",
    "\n",
    "Each executor is a separate Java Virtual Machine (JVM) process<br />\n",
    "of which it is possible to configure the number of CPUs and the quantity of\n",
    "memory allocated to it. <br />\n",
    "Only one task can process one data split at a time.*\n",
    "\n",
    "In both environments (Local and Cloud) we will therefore use **Spark**\n",
    "and we will exploit it through python scripts using **PySpark**.\n",
    "\n",
    "In the <u>local version</u> of our script we **simulate\n",
    "distributed computing** in order to validate that our solution works.<br />\n",
    "In the <u>cloud version</u> we **perform operations on a machine cluster**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfert Learning\n",
    "\n",
    "**Transfer learning** consists of\n",
    "to use the knowledge already acquired\n",
    "by a trained model (here **MobileNetV2**) for\n",
    "adapt it to our problem.\n",
    "\n",
    "We're going to provide the model with our images, and we're going to\n",
    "<u>recover the penultimate layer</u> of the model.\n",
    "Indeed the last model layer is a softmax layer\n",
    "which allows the classification of images which we do\n",
    "we do not want in this project.\n",
    "\n",
    "The penultimate layer corresponds to a **vector\n",
    "reduced** in dimension (1,1,1280).\n",
    "\n",
    "This will make it possible to create a first version of the engine\n",
    "for classification of fruit images.\n",
    "\n",
    "**MobileNetV2** was selected for its <u>speed of execution</u>,\n",
    "particularly suitable for processing a large volume\n",
    "of data as well as the <u>low dimensionality of the vector\n",
    "of output characteristic</u> (1,1,1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working environment\n",
    "\n",
    "We carried out this project on a macOS operating system. <br>\n",
    "Its installation is quickly explained in order to have a memory aid for later use.<br>\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "brew install openjdk@11\n",
    "brew install python\n",
    "brew install apache-spark\n",
    "```  \n",
    "\n",
    "Add Spark to the environment variables by adding the following lines to your <br>\n",
    ".bash_profile, .zshrc or equivalent depending on your shell: <br>\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```  \n",
    "<br>\n",
    "Reload your shell configuration file: <br>\n",
    "\n",
    "```bash\n",
    "source ~/.zshrc  # or source ~/.bash_profile\n",
    "```  \n",
    "\n",
    "#### If you have trouble with <u>Error: Heap size Spark</u> : <br>\n",
    "\n",
    "```bash\n",
    "sudo code /usr/local/Cellar/apache-spark/3.5.1/libexec/conf/spark-defaults.conf\n",
    "```  \n",
    "and add :\n",
    "\n",
    "```bash\n",
    "spark.driver.memory 15g\n",
    "spark.driver.maxResultSize 2g\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import PCA\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pyspark.sql.functions import element_at, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting PATHs to load images and save results\n",
    "\n",
    "In this local version we assume that the data\n",
    "are stored in the same directory as the notebook.<br />\n",
    "We only use an extract of **300 images** to process in this\n",
    "first version locally.<br />\n",
    "The extract of images to load is stored in the **Sample** folder.<br />\n",
    "We will record the result of our treatment\n",
    "in the \"**Results_Local**\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Sample'\n",
    "PATH_Result = PATH+'/data/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        /ai-cloud-computing-spark/train\n",
      "PATH_Data:   /ai-cloud-computing-spark/train/data/Sample\n",
      "PATH_Result: /ai-cloud-computing-spark/train/data/results\n"
     ]
    }
   ],
   "source": [
    "PATHX = '/ai-cloud-computing-spark/train'\n",
    "PATH_DataX = PATHX+'/data/Sample'\n",
    "PATH_ResultX = PATHX+'/data/results'\n",
    "print('PATH:        '+\\\n",
    "      PATHX+'\\nPATH_Data:   '+\\\n",
    "      PATH_DataX+'\\nPATH_Result: '+PATH_ResultX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the SparkSession\n",
    "\n",
    "The Spark application is controlled using a driver process called **SparkSession**. <br />\n",
    "<u>A **SparkSession** instance is how Spark executes user-defined functions <br />\n",
    "throughout the cluster</u>. <u>A SparkSession always corresponds to a Spark application</u>. <br>\n",
    "\n",
    "To avoid session creation bugs we stop all previously created sessions. <br>\n",
    "\n",
    "<u>Here we create a spark session by specifying in order</u>:\n",
    " 1. a **name for the application**, which will be displayed in the Spark web UI \"**P8**\"\n",
    " 2. that the application must run **locally**. <br />\n",
    "   We don't define the number of cores to use (like .master('local[4]) for 4 cores to use), <br />\n",
    "   so we will use all available cores in our processor.<br />\n",
    " 3. an additional configuration option allowing the use of the **\"parquet\" format** <br />\n",
    "   which we will use to save and load the result of our work.\n",
    " 4. want to **get an existing spark session** or if none exist, create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 11:15:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if SparkSession.builder.getOrCreate().sparkContext:\n",
    "    SparkSession.builder.getOrCreate().sparkContext.stop()\n",
    "    \n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('ai-cloud-computing-spark')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading data\n",
    "\n",
    "Images are loaded in binary format, which offers,\n",
    "more flexibility in how to preprocess images. <br>\n",
    "Before loading the images we specify that we want to load\n",
    "only files with **jpg** extension. <br>\n",
    "We also indicate to load all possible objects contained\n",
    "in the subfolders of the communicated folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Display of the first images containing</u>:\n",
    " - the image path\n",
    " - the date and time of its last modification\n",
    " - its length\n",
    " - its content encoded in hexadecimal value\n",
    " - its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|                path|    modificationTime|length|             content|         label|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5656|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5627|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5613|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5611|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5611|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5606|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5606|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5602|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5599|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5597|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5594|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5591|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5589|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5584|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5584|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5580|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5576|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5575|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5574|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "|file:/Users/gaeld...|2024-05-30 10:56:...|  5572|[FF D8 FF E0 00 1...|Apple Braeburn|\n",
      "+--------------------+--------------------+------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)\n",
    "df = df.withColumn('label', element_at(split(df['path'], '/'),-2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data type for each column in the Spark Dataframe</u> <br>\n",
    "\n",
    "- path: Character string (can be null)\n",
    "- modificationTime: Timestamp (can be null)\n",
    "- length: Long integer (can be null)\n",
    "- content: Binary data (can be null)\n",
    "- label: Character string (can be null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA\n",
    "\n",
    "In the introduction, we presented our objective of reducing the size of the data. <br>\n",
    "To do this we must carry out a PCA on the characteristics of the images. <br>\n",
    "<br>\n",
    "The Spark documentation tells us that PCA ([Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)) are only possible on dense vectors or sparse vectors. <br>\n",
    "- Dense Vector : explicitly stores all of its elements.\n",
    "- Sparse Vector : stores only non-zero elements and their indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA are very slow with Spark, to know the number of main components to keep in our training we carry out a PCA <br> in another notebook to visualize how many PCAs we will keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>We can see, cummulative variance can be explain by only 31 components.</u>\n",
    "\n",
    "![PCA Graph](img/PCA_grap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use these 31 components the image is simplified to improve the model's understanding of the features. <br>\n",
    "Here is a visualization of the original image and the image simplified by 31 components. <br>\n",
    "\n",
    "![simplified Image](img/simplified_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

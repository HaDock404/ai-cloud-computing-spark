{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SparkSession.builder.getOrCreate().sparkContext:\n",
    "    SparkSession.builder.getOrCreate().sparkContext.stop()\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('test2-spark')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        /Users/gaeldelescluse/Documents/OpenClassRooms/2.Projets/Projet11/ai-cloud-computing-spark/train\n",
      "PATH_Data:   /Users/gaeldelescluse/Documents/OpenClassRooms/2.Projets/Projet11/ai-cloud-computing-spark/train/data/Test1\n",
      "PATH_Result: /Users/gaeldelescluse/Documents/OpenClassRooms/2.Projets/Projet11/ai-cloud-computing-spark/train/data/results\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Test1'\n",
    "PATH_Result = PATH+'/data/results'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test2-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8547f6ae60>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5656|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5627|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5613|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5611|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5611|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5606|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5606|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5602|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5599|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5597|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5594|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5591|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5589|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5584|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5584|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5580|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5576|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5575|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5574|[FF D8 FF E0 00 1...|\n",
      "|file:/Users/gaeld...|2021-09-12 19:25:42|  5572|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(content=bytearray(b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x01\\x01\\x01\\x02\\x01\\x01\\x01\\x02\\x02\\x02\\x02\\x02\\x04\\x03\\x02\\x02\\x02\\x02\\x05\\x04\\x04\\x03\\x04\\x06\\x05\\x06\\x06\\x06\\x05\\x06\\x06\\x06\\x07\\t\\x08\\x06\\x07\\t\\x07\\x06\\x06\\x08\\x0b\\x08\\t\\n\\n\\n\\n\\n\\x06\\x08\\x0b\\x0c\\x0b\\n\\x0c\\t\\n\\n\\n\\xff\\xdb\\x00C\\x01\\x02\\x02\\x02\\x02\\x02\\x02\\x05\\x03\\x03\\x05\\n\\x07\\x06\\x07\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xff\\xc0\\x00\\x11\\x08\\x00d\\x00d\\x03\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1f\\x00\\x00\\x01\\x05\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x10\\x00\\x02\\x01\\x03\\x03\\x02\\x04\\x03\\x05\\x05\\x04\\x04\\x00\\x00\\x01}\\x01\\x02\\x03\\x00\\x04\\x11\\x05\\x12!1A\\x06\\x13Qa\\x07\"q\\x142\\x81\\x91\\xa1\\x08#B\\xb1\\xc1\\x15R\\xd1\\xf0$3br\\x82\\t\\n\\x16\\x17\\x18\\x19\\x1a%&\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xc4\\x00\\x1f\\x01\\x00\\x03\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x11\\x00\\x02\\x01\\x02\\x04\\x04\\x03\\x04\\x07\\x05\\x04\\x04\\x00\\x01\\x02w\\x00\\x01\\x02\\x03\\x11\\x04\\x05!1\\x06\\x12AQ\\x07aq\\x13\"2\\x81\\x08\\x14B\\x91\\xa1\\xb1\\xc1\\t#3R\\xf0\\x15br\\xd1\\n\\x16$4\\xe1%\\xf1\\x17\\x18\\x19\\x1a&\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xfd\\xfc\\xa2\\x8ad\\x92\\xc7\\x14m4\\xae\\x15T\\x12\\xcc\\xc7\\x00\\x01\\xdc\\xd0\\x00\\xdb\\x94rGOJ\\xf0\\xff\\x00\\xdb\\x13\\xfe\\n\\x03\\xfb9\\xfe\\xc5>\\x1cMO\\xe2\\xc7\\x89\\x9emR\\xf1d\\xfe\\xcc\\xf0\\xf6\\x93\\x1f\\x9fyt\\xcb\\x81\\xf7\\x17\\x94@\\xcc\\x80\\xb9\\xe1w\\x82k\\xc2\\x7f\\xe0\\xa0\\xff\\x00\\xf0S\\xedo\\xc0\\x177_\\x03\\x7fe;\\xc8\\x9b\\xc5P\\\\\\xacz\\xcf\\x8a\\xa6\\x829\\xed\\xf4\\xd8\\xca\\xab\\x81n\\xad\\xb9&\\x91\\xb7\\x00K\\x82\\xa8\\x01\\x1bX\\xb0+\\xf9\\xd5\\xaf\\xf8U\\xf5\\x1d^\\xe7\\xc5>%\\xd5.\\xb5\\x8d{P\\x94\\xcf\\xaaj\\x9a\\x9d\\xcb\\xcf4\\xf3\\x13\\xbb,\\xeeI<\\xf4\\xc600\\x07\\x00\\n\\xf9\\x0c\\xf7\\x8b0\\xd9ct\\xa8.y\\xad\\xfb/\\xf3~G\\xde\\xf0\\xdf\\x04W\\xcc\\xd4k\\xe3\\x1b\\x857\\xb2_\\x14\\x97\\x7f\\xee\\xaf7\\xab\\xe8\\xba\\x9e\\x85\\xfbY\\xff\\x00\\xc1\\xc3\\x7f\\xb6\\x7f\\x88.\\x1a\\xc3\\xf6z\\xf8\\x7f\\xa6x\\x03O\\xfbG\\x9di\\x7f\\xa8X%\\xf5\\xec\\xb0\\x8e\\x0cR\\t\\x83D2pw\"\\x83\\xef_\\x0c\\xf8\\xdf\\xf6\\xf4\\xff\\x00\\x82\\x84x\\xdbN\\xd4\\xed\\xbcW\\xfb_|L\\xbb\\xb0\\xd4\\xd5\\x86\\xa1\\xa7\\xbf\\x8b\\xef\\r\\xab#\\xf5\\x8f\\xca\\xf36\\x04\\xff\\x00g\\x18\\xafe\\xf8\\xc1\\xe0\\r:\\xe2\\x03o\\xa6H\\xb72\\xcd\\x1eRN\\xb8\\xf5^k\\xc6uO\\x86:\\xbf\\x864\\xbb\\x8d\\xf7\\xb2\\x0b{\\xb8\\xc1\\x9a\\x08\\xd8\\x85*:n\\x19\\xe7\\x1e\\xf5\\xf1\\xbf\\xeb>;\\x13\\xefT\\x9b^[/\\xc0\\xfd\\xb3+\\xe1\\x8e\\x17\\xc1\\xe1\\x12\\xa7\\x87\\x873\\xd3\\xde\\\\\\xcd\\xfc\\xe4\\x9f\\xe8x\\xfc\\x9f\\x16><\\xe8\\x17qx\\x97F\\xf8\\xb5\\xe2KG\\x17\\x19\\x82\\xee\\r^Tq*`\\xe4\\x10\\xd9\\xc8\\xf9Mv\\xba7\\xfc\\x15\\x1b\\xfe\\nq\\xe1o\\x13\\xd9\\xf8\\xae\\xdb\\xf6\\xe8\\xf8\\xa7<\\x96W+,6\\x9a\\x8f\\x8do.-\\xa5 \\xe4\\t \\x92B\\x92)\\xee\\xac\\x08#\\x8cW\\x1b\\xe2\\x1f\\x0e\\xfd\\x83X\\x92#\\x02\\xcc\\xab0*\\xac\\xa4eH\\xcf>\\xd8\\xc5g\\xdexMf\\xb9/\\xb7\\x1b\\x9c\\x98\\xc0\\x07j\\x8fA\\x9c\\xe6\\xbdj9\\x9e%Y\\xf3\\xbf\\xbd\\x9d\\xd3\\xe1l\\x8f\\x11x\\xfb\\x085\\xfe\\x18\\xff\\x00\\x95\\xcf\\xb7~\\x07\\x7f\\xc1\\xd1\\x1f\\xf0Q\\xbf\\x87\\x1a\\xad\\xdd\\xf7\\xc6+?\\x08\\xf8\\xff\\x00O\\xca\\xee\\xb5\\xbd\\xd1\\xe2\\xd3\\xe4\\xb7M\\xcb\\xb9\\xa26\\x82=\\xcd\\x8c\\x8f\\x9f#\\xe6\\xe9_\\xac\\x7f\\xf0N\\x8f\\xf8.\\'\\xece\\xff\\x00\\x05\\n\\xb1\\x83\\xc3\\xda\\'\\x8a\\xa1\\xf0w\\x8e\\x19\\x99_\\xc1>\"\\xbeD\\xb9\\x97lfFx\\x1b\\x85\\x99\\x02\\xa9$\\x8e\\x9d+\\xf9\\xa5\\xf1_\\x83\\x9a\\xcc-\\xa8\\x88\\x06\\x8f\\xfdf\\x7f\\x88\\x9a\\xe0|A\\xa7]hw\\xc9\\xa8i7\\x06\\x0b\\x9bw\\x12\\xc1<\\x04\\xa3\\xc6\\xe0\\xe42\\x91\\xc8 \\xf4=E}\\x0e\\x039\\xab)%7t~w\\xc4\\x9e\\x1d`=\\x93\\xab\\x82\\\\\\x8f\\xb2\\xdb\\xee\\x7f\\xa1\\xfd\\xb3\\x9d\\xdd\\x8d/^\\r~*\\xff\\x00\\xc1\\x00\\xbf\\xe0\\xe0\\xa9\\xfe#>\\x91\\xfb\\x0e\\xfe\\xdd\\x9e3\\xdf\\xe2\\x15\\xdbg\\xe0_\\x88\\x1a\\x9c\\xe3v\\xa6\\xa3\\x84\\xb3\\xbd\\x90\\xfd\\xe9\\xc7\\n\\xb3\\x9ed\\xe3~[.\\xdf\\xb58;\\xb3\\xed_MN\\xa4jG\\x9a\\'\\xe2X\\x9c5l%gJ\\xaa\\xb3B\\xd1E\\x15\\xa1\\x88\\xc3\\xc0\\xc89\\xf5\\xaf\\x93\\xff\\x00\\xe0\\xa0\\xdf\\xb5\\xb6\\xbd\\xe0\\xbd\\x12o\\x85\\x1f\\x065{c\\xa9]\\xdb\\xcd\\x06\\xbb~`\\xf3>\\xc6\\x8c\\x00\\x0b\\x1bn\\xdb\\xe6cp \\xab\\x01\\x9cpA\\x03\\xd6?j\\xbf\\x8c\\x8d\\xf0\\xdf\\xc2\\x03\\xc3Z!\\x93\\xfb_Y\\x86H\\xe1\\x96\\xde\\xe9\\x11\\xec\\xa3\\xc73\\x10r\\xde\\xa1p\\xa4\\x12\\x08\\xc8\\xaf\\x80|q\\x1c\\x1a\\x86\\xb4\\x16\\xea\\xe82\\xa1&Eb[\\xafRy\\xc9\\xfeu\\xf9\\xff\\x00\\x1aq4\\xb2\\xd8\\xac\\x0e\\x19\\xfe\\xf2kW\\xfc\\xab\\xfc\\xdf\\xe0\\xb5>\\xc3\\x84rHf\\x18\\xc5^\\xba\\xbc\"\\xefn\\xed~\\x87\\x8a\\xf8G\\xc1\\xbaV\\x8c\\xf2.\\xaedl\\x8f\\xf5\\x811\\xbf\\'\\xa9\\xe7$\\xfe\\x1d\\xea=[J\\xf0\\xd5\\x9d\\xf3\\x18\\xf5\\\\\\xa9g\\x11*\\x00\\xcc\\xccT\\xe0a\\x8f\\'\\xb7\\xb5u_\\x12\\xfcQ\\xe1\\xfd\\x12\\xdfe\\xa2$\\xa4\\x0c\\x04\\xf3Dg \\x13\\xc6}@\\x18\\x1dy\\xf7\\xaf\\x8c\\xbfh\\xff\\x00\\x8f\\x1a\\xbe\\x9b\\xae\\xcb\\x0e\\x96\\x8fk\\x98Y\\x8a\\t\\x86Q\\xc7\\xe3\\xd4q\\xef_\\x9c\\xe1\\xe9U\\xab\\xef\\xee~\\xfb\\x82\\xc3\\xcf\\x15&\\xe2\\xf9n\\xbf\\xad\\x0e\\xab\\xe2\\xdf\\xc6M\\n\\rxh\\xb6\\xea!\\xf2\\x86$Y\\xe1#\\'8\\xe4\\xe3#\\x8fJ\\xf1?\\x8a\\x7f\\x12\\xef\\xe6\\x8dt\\xa9\\'kx\\xe5\\xf9\\x8cQ\\xa9\\x19\\x8d\\x86U\\x83s\\x90s\\xd2\\xbc\\xeb_\\xf8\\xaf\\xa9\\xf8\\xc7]\\xfb_\\x88\\xef\\xdc\\xbc\\xa7\\x0btA\\x1c\\xfb\\xff\\x00\\x8dwZW\\x84\\xf4\\xdf\\x16X\\xda\\xdf]]\\x15t\\x01J\\xbb\\xe4\\x1c\\x1e\\xd5\\xe8\\xbc$i\\xc9T\\xaa\\x8c\\xf1\\xf9\\x96\\x1b)\\x94\\x15M_\\xdf\\xaf\\x91\\xcf]\\xe8Z\\xcf\\xfc \\xfa\\x8f\\x8a\\xa7\\xd3\\x12\\xe9c\\xb7P\\x82iY\\tb\\x02\\xe7=NB\\x8e\\x07>\\x98\\xe6\\xb4\\xbfd\\x9f\\x0f\\xf8\\x9b\\xe2\\xc5\\x95\\xe7\\x83\\xb5\\x9f\\n\\x19$\\xb7\\x98\\x98\\xef\\x923\\x98\\x90\\x93\\x85\\xf7\\xda0\\xa3\\x9frMw_\\x1d\\xa3\\xd3\\xfc3\\xe1?\\t\\xf82\\xc2\\xc5\\xe4\\x97Qc4p\"\\x1d\\xcd\\x93\\xb5\\t\\xf5\\x00d\\xfe5\\xf6\\x97\\xec\\x91\\xf0\\x17\\xc1_\\x06\\xbe\\x0f\\xc3\\xad\\xea\\xb6\\xca5=B\\xdf(<\\xb1\\xb9\\x98\\x8e\\xa7>\\x995\\xd1\\x85\\xc7{H\\xf28\\xd96\\xed\\xe8\\x8f\\x92\\\\I\\x89\\xa9\\x98\\xa9Ro\\x9aZ%\\xd1/N\\xe7\\xc1_\\x1b\\xfe\\x04\\x0f\\x00\\xac\\xb2\\x89n\\x19\\x8a\\xf3+\\xc1\\xf2\\xee?\\xc3\\xbb>\\x80\\xf6\\xff\\x00\\x11\\xf3\\x07\\xc4\\x1b\\x00/\\x08R\\x17\\xfb\\xc1}k\\xf4\\'\\xf6\\xd4\\xd05\\xe9\\xfc\\xd6\\xb4\\xd3\\xdd\\x81\\x94\\x91 \\x07\\xa7>\\xbcW\\xc3_\\x12\\xbc\\x1b\\xaaA,\\x972Z\\xb0\\x93q,\\x18\\x8c\\xfeU\\xeaa\\xaa\\xbau-s\\xf5l57\\x8b\\xcb9\\xaa;\\xb6\\x8f\\x12\\xd5\\xaeo4\\xbb\\xb5\\xbd\\xb0\\xb9\\x92\\x1b\\x8by\\x84\\x90\\xcd\\x19\\xc3#\\x03\\x90A\\xecA\\xaf\\xe9\\xd3\\xfe\\r\\xd0\\xff\\x00\\x82\\xba\\x1f\\xf8(w\\xec\\xdb\\xff\\x00\\nc\\xe2\\xd4\\x90E\\xf1?\\xe1\\xad\\x85\\xbd\\x96\\xa9p\\x93\\x1ck\\x96\\x01v[\\xdf\\x00\\xec[\\xce*\\x9bf\\xc1`dS\\'\\xca$\\x08\\xbf\\xcc\\xef\\x8c\\xac\\xaeC5\\xdc\\xcb\\xf3\\xb9\\xe7#\\x195\\xec\\xbf\\xf0H\\x8f\\xdbk\\xc4\\xdf\\xb0O\\xed\\xeb\\xe0\\x9f\\x8b\\xfaM\\xcc\\x83L\\xba\\xd5#\\xd2\\xbcGj.\\x16$\\xb9\\xb1\\xb8uGWv\\xe1T\\x1d\\xaeI\\xe8\\x12\\xbe\\xf3+\\xc4sF\\xd7?\\x9e\\xb8\\xef)\\xf6X\\x97Q-\\x8f\\xec\\x82\\x8a\\xab\\xa7j\\x1a~\\xb7\\xa7[\\xeb:E\\xec\\x17v\\x97p$\\xd6\\xb7V\\xd2\\x89#\\x9a6\\x00\\xab\\xa3\\x03\\x86R\\x08 \\x8e\\x089\\xa2\\xbd\\xd3\\xf3#\\xe1\\xff\\x00\\xda\\xcb\\xe2~\\xb5\\xe2\\x8f\\x88:\\xe1\\xd7c\\x82\\xd6=\\x16\\xf6};O\\x10\\xf50\\xc5+\\r\\xcc{\\x969lv\\xce+\\xe5\\xd7\\xf1\\x8b\\xf8\\x9b\\xc4\\x9f\\xf0\\x8c\\xe9\\x98\\x96i\\x19\\x88dS\\x8e3\\xd4\\x8e\\x95\\xd6\\xfcH\\xf8\\xbbi\\xe2\\xcb\\xedOW\\xf1\\x8d\\xc1\\xb8\\xbb\\xd4.\\xday\\x928\\xc2\\xefwl\\xb1\\n8\\x1c\\x9e\\x95\\xad\\xf0O\\xc0\\xde\\n7\\xe7\\xc4\\x16\\xf7J0\\xc5N\\xdc\\x10\\x1b\\xbes\\xdf\\xfck\\xf9\\xd3\\x17^9\\x86}R\\xbd{\\xdaR\\x7fu\\xf4\\xfc\\x12?x\\xc8p\\xb4r\\x9c\\xb5:\\x89\\xa7e\\xe9{\\x7f\\x99\\xf2\\xb7\\xc7v\\xd7\\xf4\\xdd3T\\xb2\\xba\\xb4\\xb6\\x8aX\\xdd\\xa4\\x8ey\\'\\n]@\\xe8\\x18\\x9e\\xbe\\x83\\xaf\\xca}\\xeb\\xf3\\xff\\x00\\xe2\\x9f\\x8bt\\xcb\\xfdF\\xea\\rJ\\xd2\\xe38fY\\xd6\\xe4\\xc8\\xc1\\xb9\\xe3\\x9fq\\xfa\\xd7\\xeb\\xf7\\xed\\xc9\\xf0\\xa3\\xc3w\\x9e\\t\\xbc\\xd6?\\xb3\\x15\\xaeJ\\x14\\x81\\xa2\\x8cn*@\\'$u\\xed\\x8fL\\x1a\\xfc\\x9b\\xf8\\xb9\\xe1\\x0f\\xf4\\xc9 \\x1a9\\xc4R\\x15\\xdc\\x90\\x80Nz\\xee`2\\xdf\\x8f\\xbd}\\x05:\\xb4a$\\xe1\\xb1\\xfa>GR\\x96a\\x82\\x94\\xe3{\\xecp>\\x1b\\xb3\\xd3\\xb5H\\x1dn\\xe0\\x17\\x11\\xb6A8\\xc3)\\xfe\\x95\\xd7h\\xde/]\\x02{m:s,\\xb1y\\xb1\\xc4\\xb0\\xa1%\\x88\\xc8\\x18\\xfa\\xe2\\xb3\\xbc\\t\\xe1\\x87\\xd25h\\xa2\\x9e5H\\x1aU23\\x81\\x85\\x1d\\xf2+\\xd4\\xed~\\x07\\xf8\"\\xfb[\\xd2\\xbcOm\\xe3E\\xd3\\xafm\\xaf\\x12x\\x83(h\\x9c\\x83\\x9c\\x10{V\\xf5*P\\x9c\\xed7\\xa1\\xf9\\x8f\\x14`\\xf3\\x18\\xe2\\x9e\\x8d\\xadm\\xbb=;\\xe1\\xe7\\x85-~5~\\xd7\\xfa\\x8c+\\x0b\\xbe\\x91\\xe0\\x8b[k\\x18Y\\xb9P\\x04jW\\xf1\\xe7\\xf1\\xc5}\\xc5\\x15\\xde\\x87\\xa3hp\\x1dL\\xa6-\\xe3\\xc4A\\xd8\\x05Q\\x8e\\x95\\xf2\\xc7\\xc3\\xe9\\xbe\\x1c\\xfc\\x03\\xd3\\xf5\\x1d_@\\xbf\\x12^k\\xf7\"\\xf7R\\xbd3\\x173\\xca#T\\x07\\x93\\xc0\\n\\x8a08\\xe3\\xdc\\xd7\\x19\\xf1s\\xf6\\x8b\\xd6<Mvc\\xd3\\xfcBv2\\x15[q&\\xe4\\xcfL\\x12:\\x1e\\xff\\x00\\x8d|\\xbe&\\x9c\\xeaV\\x8d\\x1a\\x0fH\\xee\\xfc\\xfa\\xfe79xk!\\xc7f8\\xcfi\\xca\\xe2\\xb6\\xbb\\xb9\\xea?\\xb4\\xf7\\xc5\\x7f\\x0e\\xde\\xda\\\\X\\xc1\\x1b\\xcd\\x19BLp0u~\\xdd\\xbb\\x8c\\x8a\\xf83\\xe2\\xce\\xb1m\\x7f\\x7fsie\\xa6\\x88\\x9e\"CD[\\x04\\x9fO\\xadv\\x7f\\x12>!\\xde\\xf8N\\xc0Iu\\xe3;[\\xa8&\\x06I-b\\xb8\\x0c\\xd10 \\xfc\\xc3\\xaa\\x93\\xd3=q\\x9a\\xf9\\xcf\\xc6\\x1f\\x17m\\xf5K\\x99dH\\x96b\\x1f\\x1b!\\xce\\xec\\x9fE\\x15\\xf4\\xb9~_Y\\xb4\\xd5\\xe4~\\xc1\\t\\xe02,\\'-j\\xa9y\\xb7o\\xc1\\x98^?\\x80[\\xc4au\\xc1e\\xdd\\x86^\\x9f\\x8dyf\\xb8E\\xbd\\xc6Cd\\x1eA\\x06\\xba\\xcf\\x1ax\\xe3S\\x88\\xb5\\xad\\xcd\\xa4\\x81\\x9b\\x81\\x14\\x8b\\xc8\\xae7]\\xb5\\xd6\\xad\"\\x86}^\\x13\\x0f\\x9e\\xbb\\xa0\\x8aC\\xf3\\x14\\xfe\\xf0\\x1e\\x99\\xcf\\xe2\\r}\\xeeUF\\xad(\\xdeG\\xe3\\\\c\\x9a`q\\xb5\\x1ciK\\x9a\\xe7\\xf5\\xf3\\xff\\x00\\x04\\x1a\\xfd\\xac?\\xe1\\xb0?\\xe0\\x97\\xbf\\r|y}\\xacAu\\xadh\\x1ay\\xf0\\xdf\\x88R\\xd2\\xdf\\xcbK{\\x8b,$q\\xe1\\xba\\x9f\\xb2\\xb5\\xab\\x128%\\xcf|\\xd1_\\xcf\\x17\\xfc\\x13\\x87\\xfe\\x0b\\x9b\\xfbY\\xff\\x00\\xc18~\\x00\\\\\\xfc\\x03\\xf8#}\\xa7G\\xa3\\xdd\\xf8\\x8e\\xe3X\\x95ntXn\\x1b\\xcf\\x96(\"c\\xb9\\xc6q\\xb6\\x04\\xe3\\xa7\\xe7E}$f\\xec~G*O\\x99\\x9f\\xa1_\\xb4\\xa4\\x1a\\xdf\\x83\\xbcY\\xa8\\xf8[[\\xb3\\x9a\\xdfQ\\xd3\\xaf\\\\^!8x\\xdd\\x0e\\x1a3\\x83\\x8e\\x08<\\xd7#\\xf0\\xe7\\xe3\\xef\\x8d\\xfc#v\\xb1\\xea\\x1a\\x93\\xc6\\x92?\\x98\"yJ\\r\\x8c\\x06\\x08\\x03\\xd4c\\x1f\\x8dz\\xef\\xfc\\x14\\x068\\xae\\xff\\x00i\\x0f\\x88v\\x89*\\xc4\\xc7\\xc5w\\xc0\\xca\\xcb\\xf7\\x7f|\\xf9\\xfa\\xd7\\xc9+qsyv\\x11/\\x9aw\\xb6v\\x92di\\x15K\\xa8<.[\\xef`\\x1e\\x00\\xecF;\\xd7\\xe1x\\x9c,(\\xe3\\xa7\\x16\\xb4Rk\\xeel\\xfe\\xb0\\xe1\\x85\\x86\\xccrZ~\\xd2)\\xde1n\\xfeim\\xf3>\\xf3?\\x1b\\xf4\\x0f\\x8c\\xbf\\r\\x8e\\x89\\xa8N&\\xb9hV\\x00\\xc5\\x02\\xaa\\x10\\x01\\x04rI\\xe79\\xe4v\\xe9\\x8a\\xf9\\xc3\\xe2\\xe7\\xc0\\xfd/H\\xb7\\xbc\\xbd\\x8e\\xc6\\x18\\xa4\\xba*\\xb3\\xcd\\xf6ea\\xb0\\x11\\x90\\xbb\\xb2A?\\xde\\xce\\xee\\xc0\\x8c\\x9a\\xe6|\\x0f\\xe3\\xc4\\xd0/R\\xebO\\xbeH\\xd5\\x87\\xfc{\\xa3\\x90\\x03z\\x96=9\\xcex\\xed]\\xe5\\xcf\\xc6k/\\x10\\x93\\xa5k\\x04\\xdc\\xab\\x05GV\\xda\\xdf6A\\x18<\\x0fNy\\xfc:\\xd7mhQ\\xaf\\x05m\\x1a\\x13\\xcb\\xaae\\xf5Z\\xa3\\xf0^\\xed#\\xe5\\xcf\\x13\\xfc\\x14\\xd4UZ\\xe7N\\xb3\\x93\\xcbdfT+\\xb7y\\xecF:\\x8cW;\\'\\xc3\\x1dvyd\\x8fK\\x06i`\\xb68\\xb5vd\\x92%=\\x1foB1\\xc8\\xc7\\xe5_Ux\\xefR\\xf0\\x8d\\xba\\xc6\\x90BZY\\x00XPJp\\x08\\xe8\\x98\\xe8\\xbe\\x9dk:\\xc3E\\xf0\\x85\\xf5\\xcc\\xfa\\x8d\\xdd\\xc2F\\xec\\xcaD\\xaa\\xb8e\\x0b\\xc7\\xcax ~5\\xcb\\ns\\xa6\\xec\\xd9\\xd5\\xf5\\xcb\\xa5)B\\xff\\x00#\\xc1\\xf5x|E\\x17\\x84!\\xd05\\x0b6\\x86K\\x14\\xf2\\xe5r\\x1by?x\\x13\\x93\\xc1\\xc3c\\x00v\\xae.\\xea\\xee\\xc2\\r6xRk\\xa8e+\\x90\\x02}\\xe6\\xf5\\xdc}\\x87#\\xdf\\xda\\xbe\\x8c\\xf8\\xd5\\xa7hPY\\xb3\\xe9\\xb7*\\xe2fID\\xcd\\x96~\\x98\\xe5\\x89>\\x9e\\xf5\\xe1>(\\xd4|>\\xb0\\xdeYk\\x97\\xea\\xb25\\xb16n\\x13!\\x98\\x1c\\x15\\xecs\\xd0\\xd5\\xc5B\\x9c\\xdf*\\xfb\\x8fC\\tV\\x840\\xb7\\x8c\\\\n\\xefe\\xb9\\xf3\\xf7\\x8d\\xfc/ic}q\\xa8\\xeaV\\xad\\xabKp\\xbb\\xa2K\\xfb\\xc9\\x8a\\xc4NF\\xe0\\xb1\\xba\\xa9\\xc7\\xa3\\x028\\xe9\\\\\\xd6\\xa7\\xa8\\xe9\\x9a~\\x80\\xbat\\xd0\\x81y\\xc8\\xd9cn\\x90\\x8fl\\xec\\x03uv\\xde?1Y\\xb2\\xad\\xa3\\x07\\x8c\\x0cH\\xc1\\x8e\\xe6\\xfa\\xff\\x00\\xf5\\xab\\xcc<Wu\\r\\xab\\xfd\\xa2\\x19\\x8e\\xf5l\\xc6\\xcb\\xc1\\x06\\xbe\\x9b\\tV\\xa5XF\\x17\\xd0\\xf9\\xcc\\xd3\\x01\\x97S\\xa9*\\xfc\\x89\\xc9\\xf5w\\x7f\\x9bv\\xf9X\\xe4\\xbcY\\xaa\\xdcOx\\xb73\\xbb<\\xc3!g\\x95\\x8b:\\xfar{\\x8a\\xe25\\x12\\xf77\\x8c\\xcf!v,~s\\xd5\\xbd\\xebs\\xc5\\x1a\\xcc\\xf7\\x97R\\xcfp\\xf9gvf\\xc0\\xc6I>\\x83\\xa5`YGqy\\x7f\\x1d\\xbd\\xb2\\x16\\x92W\\n\\x8a\\x06I\\'\\x81_a\\x82\\xa6\\xe3\\x04\\xcf\\xc5x\\x8f\\x11NuZ\\x89\\xf6\\xbf\\xec\\x01\\xff\\x00\\x04n\\xfd\\xab\\x7fo\\x1f\\x81\\xb3\\xfcj\\xf85\\xe0\\x99\\xef\\xf4\\x8bmz}-\\xa7D\\\\y\\xd1E\\x0c\\x8c:\\xfaL\\xbf\\x9d\\x15\\xfd\\x1f\\x7f\\xc1\\x0e\\xbfc\\xe8\\xbfb\\x8f\\xf8&\\xbf\\xc3\\xdf\\x86w\\x93+\\xea\\xda\\xdd\\x80\\xf1\\x1e\\xba\\xf0\\xca^3sz\\x88\\xea\\x17(\\xa4b\\x15\\x85H \\xe1\\x95\\xb9#\\x14W\\xaf\\xc8\\x8f\\x82\\x94\\xdf3\\xd4\\xf9\\x8f\\xfe\\x0bU\\xf0\\x06\\x7f\\x86\\xbf\\x11\\xa2\\xf8\\xb7\\xa0[\\x88\\xb4\\xaf\\x16\\x86i\\xca\\x05\\x02;\\xe5\\x19\\x90cqf/\\xfe\\xb0\\xb6\\x00\\xcb\\x10:W\\xe6\\x87\\x89\\xa5\\x9fG\\xd4\\xda\\xe2\\xe3\\xe6t\\xe5\\x15x\\x07#\\x01\\x87\\xe5_\\xd0\\x9f\\xfc\\x14\\x0f\\xf6b\\x8f\\xf6\\xa6\\xfd\\x9d5_\\x07i\\x96\\t6\\xbdc\\x1b^x}\\xca\\xae\\xff\\x00\\xb4(\\xff\\x00V\\x19\\x88\\n\\x1c\\r\\xa4\\xd7\\xf3\\xfb\\xf1\\x8b\\xc3\\xda\\xc6\\x8b\\xe2\\x1b\\xed\\x03\\xc4Zd\\xb6Z\\x9d\\x9d\\xd3\\xda]\\xda\\xdc\\xa1I-\\xe4\\x8d\\x99\\x19\\x18\\x1f\\xbaA\\x04\\x10y\\x18\\xc7j\\xfc\\xf3\\x89\\xb2\\xd7G\\x1e\\xea\\xc5{\\xb3\\xd7\\xe7\\xd7\\xfc\\xcf\\xe8\\x0f\\x0b\\xb3\\xfaur\\xef\\xaa\\xd4~\\xf5=>]\\x1f\\xe9\\xf29\\x1b\\x1f\\x19j\\x16z\\xcc\\x8b\\r\\xe3\\xca\\x1ffCc\\x04w\\xc9\\xf5\\xcf\\xb7\\xadt\\xda\\x17\\x8fe\\xd3n\\xa4\\xf1\\x16\\xa5 \\x10\\x08\\xdc\\xc0\\x80\\xe4\\xb4\\x8b\\x8f\\x94\\x01\\xc8\\xe4\\x8eMy\\xe5\\xce\\xa7.\\x9d\"5\\xd5\\xacD h\\x91\\xa2\\xc6\\xe2\\xc0\\xe7q\\xc7_\\xbd\\xd7\\xbe=\\xaa;\\x9f\\x12\\xc7g\\x02\\xc4Y\\x89\\xdf\\xbdde\\xc7\\xd4s\\xda\\xbem\\xd2\\x95\\xd2G\\xebU\\xeb\\xd2\\xab\\x1d\\xbc\\x8e\\xfa\\xf7\\xe2f\\xbf\\xe3\\x1b\\x95\\x8a\\xcd\\x0c\\x93\\x99\\xb7[\\xc7\\x1a\\x92\\xdea\\xe3\\'\\xf3\\xaf_\\x9f\\xc2\\x1a\\x8d\\xaf\\x84m\\x16\\xeaW\\xfbQ\\xb7\\x06u^\\x0b1\\xe7>\\xd8\\xce=\\xf1\\x9fj\\xf1?\\xd9\\x86\\xfbM\\xd5>&$\\xf7\\x0eF\\xd0X\\x0cq\\x9a\\xfa[\\xc7\\x1a\\xbd\\x9d\\xdd\\x87\\x90S\\n\\xaa\\x17r\\x9e@\\xaf\\x1f\\x1f\\x88\\xabJ\\xbf\\xb3G\\xc5\\xe7Y\\x9f\\xd5q\\xb4\\xe8R\\x8d\\x96\\xf7\\xf5<G\\xc6\\xd2j\\xd0i\\x83N\\x9eu\\x0c#\\x91VH\\xd4\\x96\\x90o\\'\\x9c\\xfag\\x15\\xf3g\\xc4\\x8dWS\\xd3\\xeeM\\x84\\xd3o\\x00\\x92\\xa1\\xbf\\x83\\x9e\\xd5\\xf5\\xb7\\xc5-\\x0e\\xc5\\xbc=\\xfd\\xa1a~\\xc1\\x02\\x1f\\x97\\x8e}\\xc6;\\xd7\\xc4\\x7f\\x16\\xfcF\\xb2\\xf8\\x8ek8\\x14\\xb1\\x8aFQ\\x83\\x92GS]\\xf9R\\x95J\\xbc\\xaf\\xd4\\xe9\\xc0g4\\xeaS\\x9f\\x91\\xcd\\xf8\\x8b\\xc4\\x13(7\\x12\\xdc\\x93\\x1e\\xed\\xb8,:\\xe3\\xd3\\xf0\\xaf3\\xf1F\\xb4\\x93^\\x14\\x8a\\xe3h\\'\\x86a\\xc0\\xad\\x7f\\x14\\xea\\x8d\\x99\\x1d\\xe6>\\xa3\\x1d\\xab\\x88\\xd6\\xef\\x8c\\xee\\\\.}\\xf3\\xd6\\xbe\\xfb-\\xc2+\\xa93\\xe3x\\x8f:\\\\\\xadE\\x99:\\xb5\\xd7\\x9e\\xd8;~P~a\\xdf\\x9a\\xfb\\x07\\xfe\\x08O\\xfb\\x08\\xeb?\\xb7_\\xed\\xf9\\xe1/\\x075\\xab\\x8d\\x13A\\xbcMg^\\xba\\x08\\x8c\"\\xb6\\x81\\xc3tr\\x03e\\xb6\\x8d\\xbdH\\'\\x83\\x83_$\\xf8w\\xc2:\\xe7\\x8c5\\xfb_\\x0fh\\xbal\\xd3\\xdc\\xdd\\xce\\xb1C\\x0cQ\\x92\\xcc\\xccp\\x00\\x1d\\xcek\\xfa\\xbb\\xff\\x00\\x83{\\xbf\\xe0\\x95\\xa9\\xff\\x00\\x04\\xe9\\xfd\\x94\"\\xf1O\\xc4M\\x12\\xea\\xcf\\xe2?\\x8e\\xe1\\x8e\\xef\\xc4\\x96w\\xa8\\xa1\\xf4\\xd8\\x14\\xb7\\x91k\\x81\\xc8m\\xa7\\xccp\\xd8ey\\x19\\x08\\x1b+\\xea\\xa8S\\xbb?\\x1a\\xcc\\xf1.Sm\\xee\\xcf\\xbf\\xedmm\\xac\\xed\\xa3\\xb4\\xb5\\xb7X\\xa2\\x89\\x02E\\x1cj\\x02\\xa2\\x81\\x80\\xa0\\x0e\\x00\\x03\\x8c{QR\\xd1]\\xe7\\x844d\\xe7 W\\xe5\\xc7\\xfc\\x16c\\xfe\\to}\\xab\\xc9\\xac\\xfe\\xd6\\xff\\x00\\x024\\xf5y\\x18\\xc9}\\xe2\\xdd\\x1c\\x18\\xe2\\x8e%T\\x89|\\xd8\\x11F\\xe7y\\x1f{\\xbf\\\\\\x92Os_\\xa8\\xc9\\xb7p\\x01\\xbbt\\xa8\\xae \\xb7\\xba\\x85\\xedn\\xa1Y#\\x91\\n\\xc9\\x1b\\xa8*\\xcaF\\x08 \\xf5\\x04v\\xaeL^\\x12\\x8e6\\x8b\\xa7Q\\x7f\\xc0g\\xa3\\x95f\\xb8\\xbc\\x9f\\x1b\\x1cN\\x1d\\xea\\xb7]\\x1a\\xea\\x99\\xfc\\xa1x\\xb6\\x19\\xa2\\x99|\\xe6t\\n\\xdbC\\xb2aw\\x8eH\\xc1\\xf4\\xcdr^\"\\xd4n\\xad\\xb5\\x08\\xa6y\\xf7\\xc2#9U\\xc6\\t\\xc1\\xc7\\xe1\\x9cW\\xef\\xff\\x00\\xed\\xff\\x00\\xff\\x00\\x04,\\xfd\\x9f\\xff\\x00jki|o\\xf0E-\\xbe\\x1f\\xf8\\xbc\\xc9,\\xb7\\x06\\xca\\xdc\\xb6\\x9f\\xa8\\x92\\x8a\\x15^\\xdf!a \\xa0\\xc3E\\xb0~\\xf1\\xcb\\xab\\x9c\\x11\\xf9!\\xfb_\\x7f\\xc1\\x1e?n\\x7f\\xd9\\x82\\xe29\\xfcQ\\xf0\\xc2MoLb\\x8b\\x1e\\xab\\xe1\\xb5k\\xb8\\x1d\\xdbq\\x11\\x9d\\xab\\xb86\\x14\\x93\\xf2\\xe0z\\xd7\\xc7b2,E\\t\\xedu\\xdd\\x1f\\xb3\\xe5\\xbc}\\x82\\xc5\\xd3I\\xcb\\x96_\\xca\\xff\\x00G\\xb1\\xe0\\x9f\\x01<b\\x9a7\\x8eZ\\xeaI\\x0cyRc\\xd8p3\\xe9_D\\xdf\\xfcM\\xb5\\xd7\\xa2\\x8eG\\xbbP\\xb1\\xc7\\xf721\\xd3\\xbd|\\xd9e\\xe0\\x9f\\x16\\xf8;Q\\x96\\xc7^\\xf0\\xf5\\xcd\\x85\\xecyY-\\xae\\xe0h\\xe4\\x88\\xf7\\x05X\\x02+N\\xf3P\\xf1-\\xb6\\x9a!\\x92I\\x04\\x84\\x05O\\x90\\x00@\\xf5\\xc0\\xe6\\xbe[2\\xca%R\\xb727\\xc6\\xe6\\x941\\xb5Ud\\xf5\\xb5\\x8e\\xf7\\xe2\\xe7\\xc5\\xfb\\x7f\\txbu\\x8a\\xf1\\xa5\\x078M\\xd9\\xc1\\xc7\\x15\\xf1?\\x8b|Ss\\xadx\\x8e\\xe2\\xea\\xf2C\\xb9\\xd9\\x9b\\x8e:\\x9c\\xfe5\\xec?\\x11t\\xdf\\x11j\\xf6Q\\xd9\\\\\\\\\\x89\\x0c\\xa3lq\\xafRk\\x93\\xf0\\xb7\\xec\\x8b\\xf1\\x8f\\xe2\\x0e\\xb4l\\xbc)\\xe1\\x0b\\xfdBp\\xbb\\xda\\x0b\\x1bG\\x99\\x952\\x06HPH\\x19#\\x9fz\\xf5\\xb2\\x1c\\xae\\x14\\x13\\x93\\xd5\\xb3\\xca\\xc5\\xe6q\\xc3\\xd0j2\\xd5\\xeex\\xce\\xafsyst^59\\x1c{U\\x8f\\x02\\xfc-\\xf1o\\xc4-z\\x1d#B\\xd1\\xe6\\x99\\xe6\\x90(X\\xe3,I\\'\\xa0\\x03\\xa9\\xaf\\xd4\\x1f\\xd9S\\xfe\\r\\x95\\xfd\\xb5~3\\xde[j\\xff\\x00\\x11\\xbc1o\\xe0\\xed+\\xcf\\xb7\\xf3\\xa7\\xd7\\xae\\x02\\xca\\xd0I\\xc9\\x96(\\x93>f\\xd5\\xe4\\xa9d9 dg\\x8f\\xd8O\\xd8\\x03\\xfe\\x08y\\xfb\\x1e\\xfe\\xc2\\xd2\\xe9\\xde6\\x83\\xc3\\xdf\\xf0\\x95x\\xd6\\xc1\\x92hu\\xfd]\\x07\\x97g8\\\\\\x17\\xb6\\x80|\\xa9\\xcf\\xcc\\xac\\xfb\\xe4R\\x01\\x0e\\x08\\xaf\\xb8\\xc3af\\x96\\xd6G\\xc2fY\\xc5)\\xcd\\xbeng\\xe4|\\x1f\\xff\\x00\\x04\\x18\\xff\\x00\\x83\\x7f-\\xb4\\xcb\\xcd#\\xf6\\xbf\\xfd\\xaf\\xfc(~\\xcd\\x11K\\xaf\\txSP\\x8b\\x9b\\xb3\\xf7\\x92\\xea\\xe1\\x0f\\xfc\\xb3\\xe8U\\x0f\\xde\\xe0\\x9c\\x82\\x05~\\xe1\\xd3F0)A;\\x88\\xafR\\x10\\x8c\\x15\\x91\\xf2\\x95\\xabN\\xbdG)\\x0bE\\x14U\\x99\\x85\\x14Q@\\x05\\x14Q@\\x1c^\\xbf\\xf0\\x1b\\xe0o\\x8b5Y\\xb5\\xcf\\x14\\xfc\\x19\\xf0\\xa6\\xa5{9&{\\xcb\\xff\\x00\\x0f[M,\\x84\\xf5%\\xdd\\x0b\\x13\\xf8\\xd7\\xe7\\x1f\\xfc\\x16\\x0b\\xf6\\x05\\xfd\\x92~\\x11\\xf8W\\xc1\\x1a\\xb7\\xc2\\xff\\x00\\x83\\xd6z\\x15\\xce\\xa5\\xaf\\\\E\\x7f-\\x8d\\xdc\\xe7\\xceA\\x12\\xb0R\\x1d\\xd8\\x0c\\x12O\\x00QEx|C\\x18\\xff\\x00e\\xd6\\xd3\\xa7\\xea\\x8e\\x9c\\xb2\\xbdu\\x89\\x85\\xa4\\xf7\\xee\\xfb3\\xf2\\xf3\\xf6\\x86\\xf0G\\x84~\\x1d\\xea\\x1aN\\xb9\\xe1\\xff\\x00\\x0f@^=R\\xcc<3\\x96d\\x944\\xe00`\\x088#\\x83\\x828\\xaf\\xe9;\\xf6|\\xf8a\\xf0\\xab\\xc2\\xbf\\x0e\\xfc=\\xe2\\xcf\\x04\\xfc+\\xf0\\xe6\\x83{{\\xe1\\x9bF\\x9am\\x1bG\\x8a\\x07\\xda\\xf0\\xc6\\xc57\\x81\\xbd\\x978\\xfb\\xccI\\xc0$\\x93\\xcd\\x14W\\x9b\\xc2\\xbf\\xee\\xf5=\\x7fC\\xd9\\xcf\\xdbr\\xa7\\xe8\\xff\\x003\\xd1h\\xa2\\x8a\\xfa\\xe3\\xe7B\\x8a(\\xa0\\x02\\x8a(\\xa0\\x0f\\xff\\xd9'))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = df.select(\"content\").first()\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_value = first_row.content\n",
    "img = Image.open(io.BytesIO(content_value)).convert('L')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 250,\n",
       " 253,\n",
       " 254,\n",
       " 251,\n",
       " 255,\n",
       " 254,\n",
       " 251,\n",
       " 255,\n",
       " 255,\n",
       " 249,\n",
       " 254,\n",
       " 252,\n",
       " 254,\n",
       " 252,\n",
       " 250,\n",
       " 251,\n",
       " 248,\n",
       " 247,\n",
       " 250,\n",
       " 211,\n",
       " 130,\n",
       " 122,\n",
       " 115,\n",
       " 104,\n",
       " 112,\n",
       " 115,\n",
       " 123,\n",
       " 132,\n",
       " 186,\n",
       " 249,\n",
       " 249,\n",
       " 250,\n",
       " 252,\n",
       " 251,\n",
       " 252,\n",
       " 254,\n",
       " 253,\n",
       " 252,\n",
       " 251,\n",
       " 253,\n",
       " 253,\n",
       " 255,\n",
       " 249,\n",
       " 254,\n",
       " 253,\n",
       " 252,\n",
       " 254,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 252,\n",
       " 255,\n",
       " 255,\n",
       " 252,\n",
       " 255,\n",
       " 249,\n",
       " 252,\n",
       " 255,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 252,\n",
       " 250,\n",
       " 245,\n",
       " 146,\n",
       " 95,\n",
       " 69,\n",
       " 76,\n",
       " 79,\n",
       " 79,\n",
       " 77,\n",
       " 65,\n",
       " 60,\n",
       " 60,\n",
       " 54,\n",
       " 57,\n",
       " 56,\n",
       " 55,\n",
       " 51,\n",
       " 69,\n",
       " 76,\n",
       " 66,\n",
       " 69,\n",
       " 58,\n",
       " 55,\n",
       " 57,\n",
       " 98,\n",
       " 199,\n",
       " 252,\n",
       " 253,\n",
       " 253,\n",
       " 255,\n",
       " 251,\n",
       " 254,\n",
       " 251,\n",
       " 254,\n",
       " 254,\n",
       " 251,\n",
       " 254,\n",
       " 254,\n",
       " 249,\n",
       " 254,\n",
       " 251,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 250,\n",
       " 253,\n",
       " 255,\n",
       " 249,\n",
       " 252,\n",
       " 254,\n",
       " 253,\n",
       " 254,\n",
       " 253,\n",
       " 251,\n",
       " 251,\n",
       " 213,\n",
       " 126,\n",
       " 94,\n",
       " 107,\n",
       " 80,\n",
       " 57,\n",
       " 50,\n",
       " 58,\n",
       " 57,\n",
       " 55,\n",
       " 55,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 47,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 39,\n",
       " 29,\n",
       " 33,\n",
       " 37,\n",
       " 45,\n",
       " 78,\n",
       " 90,\n",
       " 89,\n",
       " 87,\n",
       " 119,\n",
       " 177,\n",
       " 254,\n",
       " 253,\n",
       " 253,\n",
       " 252,\n",
       " 255,\n",
       " 250,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 252,\n",
       " 251,\n",
       " 251,\n",
       " 205,\n",
       " 148,\n",
       " 126,\n",
       " 112,\n",
       " 69,\n",
       " 55,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 60,\n",
       " 58,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 54,\n",
       " 54,\n",
       " 52,\n",
       " 50,\n",
       " 45,\n",
       " 47,\n",
       " 45,\n",
       " 49,\n",
       " 44,\n",
       " 44,\n",
       " 41,\n",
       " 38,\n",
       " 45,\n",
       " 40,\n",
       " 35,\n",
       " 43,\n",
       " 37,\n",
       " 40,\n",
       " 33,\n",
       " 32,\n",
       " 54,\n",
       " 71,\n",
       " 103,\n",
       " 114,\n",
       " 161,\n",
       " 234,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 252,\n",
       " 252,\n",
       " 255,\n",
       " 253,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 253,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 252,\n",
       " 252,\n",
       " 254,\n",
       " 253,\n",
       " 251,\n",
       " 194,\n",
       " 157,\n",
       " 120,\n",
       " 83,\n",
       " 64,\n",
       " 66,\n",
       " 65,\n",
       " 64,\n",
       " 59,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 60,\n",
       " 61,\n",
       " 59,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 55,\n",
       " 56,\n",
       " 48,\n",
       " 50,\n",
       " 47,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 41,\n",
       " 41,\n",
       " 40,\n",
       " 39,\n",
       " 43,\n",
       " 43,\n",
       " 41,\n",
       " 38,\n",
       " 40,\n",
       " 38,\n",
       " 37,\n",
       " 33,\n",
       " 33,\n",
       " 37,\n",
       " 69,\n",
       " 118,\n",
       " 160,\n",
       " 222,\n",
       " 251,\n",
       " 253,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 250,\n",
       " 255,\n",
       " 253,\n",
       " 243,\n",
       " 198,\n",
       " 164,\n",
       " 110,\n",
       " 73,\n",
       " 73,\n",
       " 81,\n",
       " 70,\n",
       " 63,\n",
       " 70,\n",
       " 69,\n",
       " 63,\n",
       " 60,\n",
       " 65,\n",
       " 64,\n",
       " 60,\n",
       " 62,\n",
       " 62,\n",
       " 58,\n",
       " 57,\n",
       " 58,\n",
       " 54,\n",
       " 55,\n",
       " 52,\n",
       " 51,\n",
       " 51,\n",
       " 49,\n",
       " 50,\n",
       " 49,\n",
       " 41,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 44,\n",
       " 38,\n",
       " 44,\n",
       " 44,\n",
       " 40,\n",
       " 35,\n",
       " 38,\n",
       " 49,\n",
       " 46,\n",
       " 34,\n",
       " 32,\n",
       " 28,\n",
       " 59,\n",
       " 119,\n",
       " 199,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 253,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 253,\n",
       " 254,\n",
       " 253,\n",
       " 253,\n",
       " 235,\n",
       " 179,\n",
       " 114,\n",
       " 59,\n",
       " 78,\n",
       " 85,\n",
       " 82,\n",
       " 88,\n",
       " 82,\n",
       " 72,\n",
       " 75,\n",
       " 71,\n",
       " 69,\n",
       " 67,\n",
       " 69,\n",
       " 67,\n",
       " 66,\n",
       " 68,\n",
       " 65,\n",
       " 60,\n",
       " 61,\n",
       " 57,\n",
       " 55,\n",
       " 59,\n",
       " 72,\n",
       " 59,\n",
       " 53,\n",
       " 51,\n",
       " 48,\n",
       " 46,\n",
       " 49,\n",
       " 48,\n",
       " 50,\n",
       " 49,\n",
       " 46,\n",
       " 47,\n",
       " 43,\n",
       " 44,\n",
       " 43,\n",
       " 41,\n",
       " 44,\n",
       " 50,\n",
       " 55,\n",
       " 44,\n",
       " 34,\n",
       " 32,\n",
       " 30,\n",
       " 28,\n",
       " 96,\n",
       " 172,\n",
       " 196,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 253,\n",
       " 252,\n",
       " 253,\n",
       " 239,\n",
       " 216,\n",
       " 179,\n",
       " 73,\n",
       " 94,\n",
       " 90,\n",
       " 94,\n",
       " 104,\n",
       " 109,\n",
       " 110,\n",
       " 98,\n",
       " 81,\n",
       " 81,\n",
       " 75,\n",
       " 72,\n",
       " 71,\n",
       " 73,\n",
       " 72,\n",
       " 74,\n",
       " 80,\n",
       " 78,\n",
       " 68,\n",
       " 61,\n",
       " 60,\n",
       " 56,\n",
       " 61,\n",
       " 86,\n",
       " 60,\n",
       " 49,\n",
       " 52,\n",
       " 51,\n",
       " 51,\n",
       " 56,\n",
       " 51,\n",
       " 49,\n",
       " 50,\n",
       " 44,\n",
       " 43,\n",
       " 49,\n",
       " 47,\n",
       " 43,\n",
       " 40,\n",
       " 42,\n",
       " 38,\n",
       " 51,\n",
       " 40,\n",
       " 33,\n",
       " 31,\n",
       " 34,\n",
       " 27,\n",
       " 26,\n",
       " 29,\n",
       " 73,\n",
       " 201,\n",
       " 216,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 254,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 252,\n",
       " 254,\n",
       " 255,\n",
       " 251,\n",
       " 239,\n",
       " 169,\n",
       " 97,\n",
       " 93,\n",
       " 103,\n",
       " 110,\n",
       " 103,\n",
       " 119,\n",
       " 118,\n",
       " 115,\n",
       " 110,\n",
       " 100,\n",
       " 91,\n",
       " 90,\n",
       " 86,\n",
       " 76,\n",
       " 69,\n",
       " 73,\n",
       " 66,\n",
       " 75,\n",
       " 105,\n",
       " 107,\n",
       " 82,\n",
       " 65,\n",
       " 62,\n",
       " 61,\n",
       " 55,\n",
       " 57,\n",
       " 52,\n",
       " 54,\n",
       " 53,\n",
       " 50,\n",
       " 54,\n",
       " 54,\n",
       " 55,\n",
       " 53,\n",
       " 48,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 48,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 44,\n",
       " 41,\n",
       " 40,\n",
       " 37,\n",
       " 34,\n",
       " 31,\n",
       " 30,\n",
       " 27,\n",
       " 32,\n",
       " 22,\n",
       " 32,\n",
       " 94,\n",
       " 219,\n",
       " 248,\n",
       " 255,\n",
       " 253,\n",
       " 252,\n",
       " 254,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 253,\n",
       " 250,\n",
       " 255,\n",
       " 254,\n",
       " 251,\n",
       " 254,\n",
       " 247,\n",
       " 246,\n",
       " 136,\n",
       " 75,\n",
       " 105,\n",
       " 101,\n",
       " 114,\n",
       " 126,\n",
       " 119,\n",
       " 120,\n",
       " 115,\n",
       " 111,\n",
       " 108,\n",
       " 105,\n",
       " 102,\n",
       " 98,\n",
       " 91,\n",
       " 84,\n",
       " 70,\n",
       " 73,\n",
       " 70,\n",
       " 69,\n",
       " 77,\n",
       " 78,\n",
       " 73,\n",
       " 71,\n",
       " 65,\n",
       " 60,\n",
       " 57,\n",
       " 60,\n",
       " 56,\n",
       " 53,\n",
       " 56,\n",
       " 62,\n",
       " 58,\n",
       " 53,\n",
       " 50,\n",
       " 50,\n",
       " 49,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 48,\n",
       " 48,\n",
       " 51,\n",
       " 51,\n",
       " 45,\n",
       " 42,\n",
       " 42,\n",
       " 41,\n",
       " 41,\n",
       " 34,\n",
       " 41,\n",
       " 34,\n",
       " 27,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 41,\n",
       " 211,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.array(img).flatten().tolist()\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def process_row(row):\n",
    "    content_value = row.content\n",
    "    img = Image.open(io.BytesIO(content_value)).convert('L')\n",
    "    return np.array(img).flatten().tolist()\n",
    "\n",
    "result_array = df.select(\"content\").rdd.map(process_row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 09:48:44 WARN TaskSetManager: Stage 3 contains a task of very large size (9628 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/28 09:48:45 WARN TaskSetManager: Stage 5 contains a task of very large size (9628 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+------+--------------------+--------------------+\n",
      "| id|                path|   modificationTime|length|             content|              pixels|\n",
      "+---+--------------------+-------------------+------+--------------------+--------------------+\n",
      "|  0|file:/Users/gaeld...|2021-09-12 19:25:42|  5656|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  1|file:/Users/gaeld...|2021-09-12 19:25:42|  5627|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  2|file:/Users/gaeld...|2021-09-12 19:25:42|  5613|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  3|file:/Users/gaeld...|2021-09-12 19:25:42|  5611|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  4|file:/Users/gaeld...|2021-09-12 19:25:42|  5611|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  5|file:/Users/gaeld...|2021-09-12 19:25:42|  5606|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  6|file:/Users/gaeld...|2021-09-12 19:25:42|  5606|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  7|file:/Users/gaeld...|2021-09-12 19:25:42|  5602|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  8|file:/Users/gaeld...|2021-09-12 19:25:42|  5599|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "|  9|file:/Users/gaeld...|2021-09-12 19:25:42|  5597|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 10|file:/Users/gaeld...|2021-09-12 19:25:42|  5594|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 11|file:/Users/gaeld...|2021-09-12 19:25:42|  5591|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 12|file:/Users/gaeld...|2021-09-12 19:25:42|  5589|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 13|file:/Users/gaeld...|2021-09-12 19:25:42|  5584|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 14|file:/Users/gaeld...|2021-09-12 19:25:42|  5584|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 15|file:/Users/gaeld...|2021-09-12 19:25:42|  5580|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 16|file:/Users/gaeld...|2021-09-12 19:25:42|  5576|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 17|file:/Users/gaeld...|2021-09-12 19:25:42|  5575|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 18|file:/Users/gaeld...|2021-09-12 19:25:42|  5574|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "| 19|file:/Users/gaeld...|2021-09-12 19:25:42|  5572|[FF D8 FF E0 00 1...|[255, 255, 255, 2...|\n",
      "+---+--------------------+-------------------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "result_rdd = spark.sparkContext.parallelize(result_array).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "schema = [\"id\", \"pixels\"]\n",
    "pixels_df = spark.createDataFrame(result_rdd, schema)\n",
    "\n",
    "# Ajout d'un ID au DataFrame original pour la jointure\n",
    "df_with_id = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Jointure des deux DataFrames sur l'ID\n",
    "final_df = df_with_id.join(pixels_df, on=\"id\")\n",
    "\n",
    "# Affichage du rsultat\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(image):\n",
    "    image = image.content\n",
    "    img = Image.open(io.BytesIO(image)).convert('L')  # Convertir en niveaux de gris\n",
    "    return np.array(img).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_vector_udf = udf(image_to_vector, ArrayType(FloatType()))\n",
    "df = df.withColumn(\"pixels\", image_to_vector_udf(df.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 09:36:29 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/1446602329.py\", line 2, in image_to_vector\n",
      "AttributeError: 'bytearray' object has no attribute 'content'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/28 09:36:29 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.0.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/1446602329.py\", line 2, in image_to_vector\n",
      "AttributeError: 'bytearray' object has no attribute 'content'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/28 09:36:29 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/1446602329.py\", line 2, in image_to_vector\nAttributeError: 'bytearray' object has no attribute 'content'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/1446602329.py\", line 2, in image_to_vector\nAttributeError: 'bytearray' object has no attribute 'content'\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.functions import vector_to_array, array_to_vector\n",
    "\n",
    "df = df.withColumn(\"features\", array_to_vector(\"pixels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 09:19:55 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\n",
      "org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "mapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n",
      "\t... 21 more\n",
      "24/05/28 09:19:55 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "mapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n",
      "\t... 21 more\n",
      "\n",
      "24/05/28 09:19:55 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o260.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o260.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 09:18:50 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "mapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n",
      "\t... 37 more\n",
      "24/05/28 09:18:50 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "mapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- array element class: \"double\"\n",
      "- root class: \"scala.collection.Seq\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n",
      "\t... 37 more\n",
      "\n",
      "24/05/28 09:18:50 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o261.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:44)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o261.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.15 executor driver): org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:44)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkRuntimeException: Error while decoding: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\nmapobjects(lambdavariable(MapObject, FloatType, true, -1), assertnotnull(cast(lambdavariable(MapObject, FloatType, true, -1) as double)), input[0, array<float>, true], Some(interface scala.collection.Seq)).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1405)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:185)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$scalaConverter$2(ScalaUDF.scala:171)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- array element class: \"double\"\n- root class: \"scala.collection.Seq\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.MapObjects_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df)\n",
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def binary_to_vector(binary_data):\n",
    "    image = Image.open(io.BytesIO(binary_data))\n",
    "    image = image.convert('L')  # Convertir en niveau de gris\n",
    "    image_array = np.array(image).flatten()  # Aplatir l'image en un vecteur 1D\n",
    "    return image_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "binary_to_vector_udf = udf(binary_to_vector, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_vectors = df.withColumn(\"features\", binary_to_vector_udf(df[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 09:06:07 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/713650167.py\", line 5, in binary_to_vector\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/28 09:06:07 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/713650167.py\", line 5, in binary_to_vector\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/28 09:06:07 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/713650167.py\", line 5, in binary_to_vector\nNameError: name 'Image' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_with_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_39280/713650167.py\", line 5, in binary_to_vector\nNameError: name 'Image' is not defined\n"
     ]
    }
   ],
   "source": [
    "df_with_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "\n",
    "def to_vector(array):\n",
    "    return Vectors.dense(array)\n",
    "\n",
    "to_vector_udf = udf(to_vector, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_vectors = df_with_vectors.withColumn(\"features\", to_vector_udf(df_with_vectors[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 00:05:09 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_18013/713650167.py\", line 5, in binary_to_vector\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/28 00:05:09 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_18013/713650167.py\", line 5, in binary_to_vector\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/28 00:05:09 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_18013/713650167.py\", line 5, in binary_to_vector\nNameError: name 'Image' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Appliquer PCA\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39mnum_components, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_with_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(df_with_vectors)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Afficher les rsultats\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/kl/3nc79ycx61v13jbqlw61nnfw0000gn/T/ipykernel_18013/713650167.py\", line 5, in binary_to_vector\nNameError: name 'Image' is not defined\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "num_components = 50\n",
    "\n",
    "# Appliquer PCA\n",
    "pca = PCA(k=num_components, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_with_vectors)\n",
    "df_pca = pca_model.transform(df_with_vectors)\n",
    "\n",
    "# Afficher les rsultats\n",
    "df_pca.select(\"pca_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column content must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.BinaryType$:binary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_pca \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/test-spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column content must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.BinaryType$:binary."
     ]
    }
   ],
   "source": [
    "pca = PCA(k=20, inputCol=\"content\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df)\n",
    "df_pca = pca_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
